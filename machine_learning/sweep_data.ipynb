{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from model.make_model import *\n",
    "import sys\n",
    "sys.path.append('/home/thoresen/Documents/DeepLearning/02456-2021-project')\n",
    "wandb.init(project='dtu-course', entity=\"freja-thoresen\")\n",
    "figure_path = '../figures/'\n",
    "\n",
    "use_cuda=False\n",
    "\n",
    "cuda=False\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "  cuda=True\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "sequence_length = 10\n",
    "epochs = 5 # 1000 epochs\n",
    "learning_rate = 0.003\n",
    "lstm_dropbout = 0.2\n",
    "linear_dropout = 0.2\n",
    "input_size = 1 # number of features\n",
    "lstm_hidden_size = 20 # number of features in hidden state\n",
    "linear_hidden_size = 40 # number of features in hidden state\n",
    "num_layers = 2 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes\n",
    "\n",
    "no_batches = 100\n",
    "no_files = 100 # no. data files\n",
    "\n",
    "wandb.config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"sequence_length\": sequence_length,\n",
    "    \"epochs\": epochs,\n",
    "    \"input_size\": input_size,\n",
    "    \"lstm_hidden_size\": lstm_hidden_size,\n",
    "    \"linear_hidden_size\": linear_hidden_size,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"no_batches\": no_batches,\n",
    "    \"no_files\": no_files,\n",
    "    \"lstm_dropbout\": lstm_dropbout,\n",
    "    \"linear_dropout\": linear_dropout,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"input_size\": input_size\n",
    "}\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'data-sweep',\n",
    "    'project': 'dtu-course',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'training_loss',\n",
    "        'goal': 'minimize'\n",
    "    }\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    'learning_rate': {\n",
    "        'values': [0.0002, 0.0004, 0.0008]\n",
    "    },\n",
    "    'sequence_length': {\n",
    "        'values': [10, 40, 80, 120, 200]\n",
    "    },\n",
    "    'epochs': {\n",
    "        'value': 40\n",
    "    },\n",
    "    'input_size': {\n",
    "        'value': input_size\n",
    "    },\n",
    "    'lstm_hidden_size': {\n",
    "        'value': 20\n",
    "    },\n",
    "    'linear_hidden_size': {\n",
    "        'value': 20\n",
    "    },\n",
    "    'num_layers': {\n",
    "        'value': 2\n",
    "    },\n",
    "    'no_batches': {\n",
    "        'values': [50, 100, 200, 400, 800]\n",
    "    },\n",
    "    'no_files': {\n",
    "        'value': no_files\n",
    "    },\n",
    "    'lstm_dropbout': {\n",
    "        'value': 0.2\n",
    "    },\n",
    "    'linear_dropout': {\n",
    "        'value': 0.2\n",
    "    },\n",
    "    'num_classes': {\n",
    "        'value': num_classes\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"dtu-course\")\n",
    "print(wandb.config)\n",
    "def sweep():\n",
    "    with wandb.init() as run:\n",
    "        config = Config(wandb.config)\n",
    "        lstm, criterion, optimizer = make_model(config)\n",
    "        train_model(lstm, criterion, optimizer, config)\n",
    "wandb.agent(sweep_id, function=sweep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}