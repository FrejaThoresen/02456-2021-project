{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17b0a6d-76f3-4e3d-aa0b-92c2d65b0fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM using raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011c513-97d6-4fcf-a197-66afdf9a2081",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7da1e-110d-41ce-824a-db83e46c5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sigmoid, Softmax\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from labelled_data.tools.load_data import data_loader\n",
    "from labelled_data.tools.load_data import data_generator\n",
    "from torch.nn.functional import normalize, binary_cross_entropy, binary_cross_entropy_with_logits\n",
    "from torch.autograd import Variable\n",
    "\n",
    "figure_path = '../figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=True\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "cuda=False\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "  cuda=True\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Raw data or chunked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chunks = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b47c-4d54-4804-aca9-975fde0dcc5b",
   "metadata": {},
   "source": [
    "# Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid, y_train, y_test, y_valid = data_loader(chunks=chunks, no_files=10)\n",
    "train_gen = data_generator(X_train, y_train, sequence_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = train_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ec1d0-7d18-49c4-b535-1cf5bd710ab3",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, lstm_hidden_size, linear_hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.lstm_hidden_size = lstm_hidden_size #hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True, dropout=0.4) #lstm\n",
    "        self.linear_1 = nn.Linear(lstm_hidden_size, linear_hidden_size) #fully connected last layer\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        #self.linear_2 = nn.Linear(linear_hidden_size, int(linear_hidden_size/2)) #fully connected last layer\n",
    "        self.linear_out = nn.Linear(int(linear_hidden_size), num_classes) #fully connected last layer\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Propagate input through LSTM\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.lstm_hidden_size, device=x.device)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.lstm_hidden_size, device=x.device)) #internal state\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h_0, c_0)) # lstm with input, hidden, and internal state\n",
    "        \n",
    "        x = x.reshape(x.shape[0]*x.shape[1], x.shape[2]) # reshaping the data for Dense layer next\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.linear_2(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear_out(x) \n",
    "\n",
    "        out = self.sigmoid(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acabf4-99dc-4b4c-a65b-834f9ae9e4f7",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ec70d-622a-46b6-8252-99c60249d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 1\n",
    "num_epochs = 200 # 1000 epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 1 # number of features\n",
    "lstm_hidden_size = 40 # number of features in hidden state\n",
    "linear_hidden_size = 40 # number of features in hidden state\n",
    "num_layers = 3 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes\n",
    "\n",
    "no_batches = 50000\n",
    "no_files = 20 # no. data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97753243-616e-4c55-be2d-ef0586931a85",
   "metadata": {},
   "source": [
    " ## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f9f2d-388d-4cf9-91a7-bd20c0577926",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(num_classes, input_size, lstm_hidden_size, linear_hidden_size, num_layers) #our lstm class\n",
    "lstm = get_variable(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1105e-b80e-4d22-8f84-52082347205b",
   "metadata": {},
   "source": [
    "## Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8e622-4d88-49b3-ae37-2e231d8b32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss() # cross validation\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "#optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2df0a-ea50-47e3-9f11-4a9ddeb35953",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_result(output):\n",
    "    output[output > 0.5] = 1.\n",
    "    output[output < 0.5] = 0.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab454a5-1235-4504-92f0-04632aaae55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_training_loss = []\n",
    "epoch_validation_loss = []\n",
    "epoch_training_acc = []\n",
    "epoch_validation_acc = []\n",
    "\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid = data_loader(chunks=chunks, no_files=no_files)\n",
    "print('training files', len(X_train))\n",
    "print('validation files', len(X_valid))\n",
    "print('test files', len(X_test))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  training_loss = 0\n",
    "  validation_loss = 0\n",
    "  training_correct = 0\n",
    "  training_all = 0\n",
    "  validation_correct = 0\n",
    "  validation_all = 0\n",
    "  \n",
    "  train_gen = data_generator(X_train, y_train, sequence_length=sequence_length)\n",
    "  test_gen = data_generator(X_test, y_test, sequence_length=sequence_length)\n",
    "  valid_gen = data_generator(X_valid, y_valid, sequence_length=sequence_length)\n",
    "\n",
    "  dict = {\n",
    "      'no_train' : len(X_train),\n",
    "      'train': train_gen,\n",
    "      'no_test' : len(X_test),\n",
    "      'test': test_gen,\n",
    "      'no_valid' : len(X_valid),\n",
    "      'valid': valid_gen,\n",
    "  }\n",
    "\n",
    "  for _ in range(len(X_valid)):\n",
    "      i_batch = 0\n",
    "\n",
    "      data, labels = valid_gen.__next__()\n",
    "      data = get_variable(data*1000)\n",
    "      labels = get_variable(labels)\n",
    "\n",
    "      while data.shape[0] > i_batch + no_batches:\n",
    "        batch_data = data[i_batch:(i_batch+no_batches)]\n",
    "        batch_labels = labels[(i_batch*sequence_length):(i_batch+no_batches)*sequence_length]\n",
    "\n",
    "        lstm.eval()\n",
    "        outputs = lstm.forward(batch_data) #forward pass\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "        validation_correct += (evaluate_result(outputs) == batch_labels).float().sum()\n",
    "        validation_all += len(batch_labels)\n",
    "        i_batch += no_batches\n",
    "\n",
    "  for _ in range(len(X_train)):\n",
    "      i_batch = 0\n",
    "      \n",
    "      data, labels = train_gen.__next__()\n",
    "      data = get_variable(data*1000)\n",
    "      labels = get_variable(labels)\n",
    "      if epoch == 0:\n",
    "        print('Number of batch loops', data.shape[0]/no_batches)\n",
    "\n",
    "      while data.shape[0] > i_batch + no_batches:\n",
    "        batch_data = data[i_batch:(i_batch+no_batches)]\n",
    "        batch_labels = labels[(i_batch*sequence_length):(i_batch+no_batches)*sequence_length]\n",
    "\n",
    "        lstm.train()\n",
    "\n",
    "        train_outputs = lstm.forward(batch_data) #forward pass\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "\n",
    "        # obtain the loss function\n",
    "        loss = criterion(train_outputs, batch_labels)\n",
    "        loss.backward(retain_graph=True) #calculates the loss of the loss function\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        training_correct += (evaluate_result(train_outputs) == batch_labels).float().sum()\n",
    "        training_all += len(batch_labels)\n",
    "\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "\n",
    "        i_batch += no_batches\n",
    "\n",
    "  validation_acc = validation_correct / float(validation_all)\n",
    "  training_acc = training_correct / float(training_all)\n",
    "  #if epoch % 10 == 0:\n",
    "  print(\"Epoch: %d, training loss: %1.5f, validation loss: %1.5f, training acc: %1.5f, , validation acc: %1.5f\" % (epoch, training_loss/len(X_train), validation_loss/len(X_valid), training_acc, validation_acc))\n",
    "\n",
    "  epoch_validation_loss.append(validation_loss/len(X_valid))\n",
    "  epoch_validation_acc.append(validation_acc)\n",
    "  epoch_training_loss.append(training_loss/len(X_train))\n",
    "  epoch_training_acc.append(training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), '../models/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_gen = data_generator(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,5))\n",
    "data, labels = train_gen.__next__()\n",
    "data = data.detach().numpy()[:,0,0]\n",
    "#plt.plot(data[int(len(data)*0.25):int(len(data)*0.75)])\n",
    "plt.plot(data[5000:50000]*1000)\n",
    "\n",
    "if labels.sum() > 0:\n",
    "#plt.plot(labels[int(len(data)*0.25):int(len(data)*0.75)]*1330)\n",
    "    plt.plot(labels[5000:50000]*0.2)\n",
    "#plt.gca().set_ylim([1300,1500])\n",
    "#plt.gca().set_xlim([1610000,len(data)-860000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, len(epoch_validation_loss)-1, len(epoch_validation_loss)), epoch_validation_loss, label='validation loss')\n",
    "plt.plot(np.linspace(0, len(epoch_training_loss)-1, len(epoch_training_loss)), epoch_training_loss, label='training loss')\n",
    "plt.gca().set_ylabel('Loss')\n",
    "plt.gca().set_xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(figure_path + 'validation.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epoch_validation_acc = [acc.cpu() for acc in epoch_validation_acc]\n",
    "epoch_training_acc = [acc.cpu() for acc in epoch_training_acc]\n",
    "test_gen = data_generator(X_test, y_test)\n",
    "\n",
    "plt.plot(np.linspace(0, len(epoch_validation_acc)-1, len(epoch_validation_acc)), epoch_validation_acc, label='validation acc')\n",
    "plt.plot(np.linspace(0, len(epoch_training_acc)-1, len(epoch_training_acc)), epoch_training_acc, label='training acc')\n",
    "plt.gca().set_ylabel('Accuracy')\n",
    "plt.gca().set_xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(figure_path + 'accuracy.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197123a2-9898-448a-9f63-798481c0a298",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09449e88-217c-491c-965d-a785019172e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = test_gen.__next__()\n",
    "#train_predict = lstm(data.cuda())#forward pass\n",
    "train_predict = lstm(data.cuda())#forward pass\n",
    "data_predict = train_predict.data.cpu().numpy() #numpy conversion\n",
    "data_predict[data_predict > 0.5] = 1\n",
    "data_predict[data_predict < 0.5] = 0\n",
    "\n",
    "dataY_plot = labels\n",
    "\n",
    "plt.figure(figsize=(10,6)) #plotting\n",
    "#plt.axvline(x=40000, c='r', linestyle='--') #size of the training set\n",
    "\n",
    "plt.plot(dataY_plot, label='Actual Data') #actual plot\n",
    "plt.plot(data_predict, label='Predicted Data') #predicted plot\n",
    "plt.title('Time-Series Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f4aab342266c55a5f356d03dbdb6a3962f7972b11ff589a1da546ab1d11a116"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fp_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
