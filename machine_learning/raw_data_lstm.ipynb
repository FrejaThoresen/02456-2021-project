{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17b0a6d-76f3-4e3d-aa0b-92c2d65b0fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM using raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011c513-97d6-4fcf-a197-66afdf9a2081",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd7da1e-110d-41ce-824a-db83e46c5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** FP-modules version 3.0.2 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfreja-thoresen\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/freja-thoresen/dtu-course/runs/1d4lo1cp\" target=\"_blank\">hardy-yogurt-20</a></strong> to <a href=\"https://wandb.ai/freja-thoresen/dtu-course\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sigmoid, Softmax\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from labelled_data.tools.load_data import data_loader\n",
    "from labelled_data.tools.load_data import data_generator\n",
    "from torch.nn.functional import normalize, binary_cross_entropy, binary_cross_entropy_with_logits\n",
    "from torch.autograd import Variable\n",
    "import wandb\n",
    "wandb.init(project=\"dtu-course\", entity=\"freja-thoresen\")\n",
    "\n",
    "figure_path = '../figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22eb5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=True\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "cuda=False\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "  cuda=True\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Raw data or chunked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chunks = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b47c-4d54-4804-aca9-975fde0dcc5b",
   "metadata": {},
   "source": [
    "# Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid, y_train, y_test, y_valid = data_loader(chunks=chunks, no_files=10)\n",
    "train_gen = data_generator(X_train, y_train, sequence_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = train_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9928762f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 500, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ec1d0-7d18-49c4-b535-1cf5bd710ab3",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, lstm_hidden_size, linear_hidden_size, num_layers, lstm_dropbout, linear_dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.lstm_hidden_size = lstm_hidden_size #hidden state\n",
    "        self.sequence_length = 1\n",
    "        self.no_batches = 50000\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True, dropout=lstm_dropbout) #lstm\n",
    "        self.linear_1 = nn.Linear(lstm_hidden_size, linear_hidden_size) #fully connected last layer\n",
    "        self.dropout = nn.Dropout(linear_dropout)\n",
    "        #self.linear_2 = nn.Linear(linear_hidden_size, int(linear_hidden_size/2)) #fully connected last layer\n",
    "        self.linear_out = nn.Linear(int(linear_hidden_size), num_classes) #fully connected last layer\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Propagate input through LSTM\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.lstm_hidden_size, device=x.device)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.lstm_hidden_size, device=x.device)) #internal state\n",
    "\n",
    "        x, (h, c) = self.lstm(x, (h_0, c_0)) # lstm with input, hidden, and internal state\n",
    "        \n",
    "        x = x.reshape(x.shape[0]*x.shape[1], x.shape[2]) # reshaping the data for Dense layer next\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.linear_2(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear_out(x) \n",
    "\n",
    "        out = self.sigmoid(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acabf4-99dc-4b4c-a65b-834f9ae9e4f7",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45ec70d-622a-46b6-8252-99c60249d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "epochs = 600 # 1000 epochs\n",
    "learning_rate = 0.003\n",
    "lstm_dropbout = 0.2\n",
    "linear_dropout = 0.2\n",
    "input_size = 1 # number of features\n",
    "lstm_hidden_size = 20 # number of features in hidden state\n",
    "linear_hidden_size = 40 # number of features in hidden state\n",
    "num_layers = 2 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes\n",
    "\n",
    "no_batches = 100\n",
    "no_files = 300 # no. data files\n",
    "\n",
    "wandb.config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"sequence_length\": sequence_length,\n",
    "    \"epochs\": epochs,\n",
    "    \"input_size\": input_size,\n",
    "    \"lstm_hidden_size\": lstm_hidden_size,\n",
    "    \"linear_hidden_size\": linear_hidden_size,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"no_batches\": no_batches,\n",
    "    \"no_files\": no_files,\n",
    "    \"lstm_dropbout\": lstm_dropbout,\n",
    "    \"linear_dropout\": linear_dropout,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"input_size\": input_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147efaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"sequence_length\": sequence_length,\n",
    "    \"epochs\": epochs,\n",
    "    \"input_size\": input_size,\n",
    "    \"lstm_hidden_size\": lstm_hidden_size,\n",
    "    \"linear_hidden_size\": linear_hidden_size,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"no_batches\": no_batches,\n",
    "    \"no_files\": no_files,\n",
    "    \"lstm_dropbout\": lstm_dropbout,\n",
    "    \"linear_dropout\": linear_dropout,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"input_size\": input_size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97753243-616e-4c55-be2d-ef0586931a85",
   "metadata": {},
   "source": [
    " ## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae8f9f2d-388d-4cf9-91a7-bd20c0577926",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(num_classes, input_size, lstm_hidden_size, linear_hidden_size, num_layers, lstm_dropbout, linear_dropout) #our lstm class\n",
    "lstm = get_variable(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1105e-b80e-4d22-8f84-52082347205b",
   "metadata": {},
   "source": [
    "## Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64a8e622-4d88-49b3-ae37-2e231d8b32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss() # cross validation\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "#optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2df0a-ea50-47e3-9f11-4a9ddeb35953",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_result(output):\n",
    "    output[output > 0.5] = 1.\n",
    "    output[output < 0.5] = 0.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c49982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.watch(lstm, criterion, log='all', log_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c200f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(config):\n",
    "    lstm = LSTM(config.num_classes, config.input_size, config.lstm_hidden_size, config.linear_hidden_size,  \\\n",
    "    config.num_layers, config.lstm_dropbout, config.linear_dropout) #our lstm class\n",
    "    lstm = get_variable(lstm)\n",
    "\n",
    "    criterion = torch.nn.BCELoss() # cross validation\n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=config.learning_rate)\n",
    "    return lstm, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ab454a5-1235-4504-92f0-04632aaae55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(lstm, criterion, optimizer, config, verbose=0):\n",
    "\n",
    "  epoch_training_loss = []\n",
    "  epoch_validation_loss = []\n",
    "  epoch_training_acc = []\n",
    "  epoch_validation_acc = []\n",
    "\n",
    "  X_train, X_test, X_valid, y_train, y_test, y_valid = data_loader(chunks=chunks, no_files=no_files)\n",
    "  print('training files', len(X_train))\n",
    "  print('validation files', len(X_valid))\n",
    "  print('test files', len(X_test))\n",
    "\n",
    "  for epoch in range(config.epochs):\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    training_correct = 0\n",
    "    training_all = 0\n",
    "    validation_correct = 0\n",
    "    validation_all = 0\n",
    "    \n",
    "    train_gen = data_generator(X_train, y_train, sequence_length=config.sequence_length)\n",
    "    test_gen = data_generator(X_test, y_test, sequence_length=config.sequence_length)\n",
    "    valid_gen = data_generator(X_valid, y_valid, sequence_length=config.sequence_length)\n",
    "\n",
    "    dict = {\n",
    "        'no_train' : len(X_train),\n",
    "        'train': train_gen,\n",
    "        'no_test' : len(X_test),\n",
    "        'test': test_gen,\n",
    "        'no_valid' : len(X_valid),\n",
    "        'valid': valid_gen,\n",
    "    }\n",
    "    valid_batches = 0\n",
    "    for _ in range(len(X_valid)):\n",
    "        i_batch = 0\n",
    "\n",
    "        data, labels = valid_gen.__next__()\n",
    "        data = get_variable(data*1000)\n",
    "        labels = get_variable(labels)\n",
    "\n",
    "        while data.shape[0] > i_batch + config.no_batches:\n",
    "          valid_batches += 1\n",
    "          batch_data = data[i_batch:(i_batch+config.no_batches)]\n",
    "          batch_labels = labels[(i_batch*config.sequence_length):(i_batch+config.no_batches)*config.sequence_length]\n",
    "\n",
    "          lstm.eval()\n",
    "          outputs = lstm.forward(batch_data) #forward pass\n",
    "          loss = criterion(outputs, batch_labels)\n",
    "          validation_loss += loss.item()\n",
    "\n",
    "          validation_correct += (evaluate_result(outputs) == batch_labels).float().sum()\n",
    "          validation_all += len(batch_labels)\n",
    "          i_batch += config.no_batches\n",
    "    train_batches = 0\n",
    "    \n",
    "    for _ in range(len(X_train)):\n",
    "        i_batch = 0\n",
    "        \n",
    "        data, labels = train_gen.__next__()\n",
    "        data = get_variable(data*1000)\n",
    "        labels = get_variable(labels)\n",
    "        if epoch == 0 and verbose == 1:\n",
    "          print('Number of batch loops', data.shape[0]/config.no_batches)\n",
    "\n",
    "        while data.shape[0] > i_batch + config.no_batches:\n",
    "          train_batches += 1\n",
    "          batch_data = data[i_batch:(i_batch+config.no_batches)]\n",
    "          batch_labels = labels[(i_batch*config.sequence_length):(i_batch+config.no_batches)*config.sequence_length]\n",
    "\n",
    "          lstm.train()\n",
    "\n",
    "          train_outputs = lstm.forward(batch_data) #forward pass\n",
    "          optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "\n",
    "          # obtain the loss function\n",
    "          loss = criterion(train_outputs, batch_labels)\n",
    "          loss.backward(retain_graph=True) #calculates the loss of the loss function#retain_graph=True\n",
    "          training_loss += loss.item()\n",
    "\n",
    "          training_correct += (evaluate_result(train_outputs) == batch_labels).float().sum()\n",
    "          training_all += len(batch_labels)\n",
    "\n",
    "          optimizer.step() #improve from loss, i.e backprop\n",
    "\n",
    "          i_batch += config.no_batches\n",
    "    if validation_all > 0:\n",
    "      validation_acc = validation_correct / float(validation_all)\n",
    "      validation_loss = validation_loss/len(X_valid)\n",
    "    else: \n",
    "      validation_acc = float('NaN')\n",
    "      validation_loss = float('NaN')\n",
    "    if training_all > 0:\n",
    "      training_acc = training_correct / float(training_all)\n",
    "      training_loss = training_loss/len(X_train)\n",
    "    else:\n",
    "      training_acc = float('NaN')\n",
    "      training_loss = float('NaN')\n",
    "    #if epoch % 10 == 0:\n",
    "    if verbose==1:\n",
    "      print(\"Epoch: %d, training loss: %1.5f, validation loss: %1.5f, training acc: %1.5f, , validation acc: %1.5f\" % (epoch, training_loss/len(X_train), validation_loss/len(X_valid), training_acc, validation_acc))\n",
    "\n",
    "    epoch_validation_loss.append(validation_loss)\n",
    "    epoch_validation_acc.append(validation_acc)\n",
    "    epoch_training_loss.append(training_loss)\n",
    "    epoch_training_acc.append(training_acc)\n",
    "    wandb.log({\n",
    "      'epoch': epoch,\n",
    "      'validation_loss': validation_loss,\n",
    "      'validation_acc': validation_acc,\n",
    "      'training_loss': training_loss,\n",
    "      'training_acc': training_acc,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b26e76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"\n",
    "    Turns a dictionary into a class\n",
    "    \"\"\"\n",
    "    #----------------------------------------------------------------------\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774809cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64adbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm, criterion, optimizer = make_model(c)\n",
    "#train_model(lstm, criterion, optimizer, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76b93107",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name': 'model-sweep',\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'training_loss',\n",
    "        'goal': 'minimize'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "262ee604",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'learning_rate': {\n",
    "        'value': 0.0003\n",
    "    },\n",
    "    'sequence_length': {\n",
    "        'value': 10\n",
    "    },\n",
    "    'epochs': {\n",
    "        'value': 100\n",
    "    },\n",
    "    'input_size': {\n",
    "        'value': input_size\n",
    "    },\n",
    "    'lstm_hidden_size': {\n",
    "        'values': [20, 40, 60, 80]\n",
    "    },\n",
    "    'linear_hidden_size': {\n",
    "        'values': [20, 40, 60, 80]\n",
    "    },\n",
    "    'num_layers': {\n",
    "        'values': [2, 3]\n",
    "    },\n",
    "    'no_batches': {\n",
    "        'value': 100\n",
    "    },\n",
    "    'no_files': {\n",
    "        'value': no_files\n",
    "    },\n",
    "    'lstm_dropbout': {\n",
    "        'values': [0.2, 0.4, 0.6]\n",
    "    },                            \n",
    "    'linear_dropout': {\n",
    "        'values': [0.2, 0.4, 0.6]\n",
    "    },\n",
    "    'num_classes': {\n",
    "        'value': num_classes\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ef6b229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: lpycscps\n",
      "Sweep URL: https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps\n"
     ]
    }
   ],
   "source": [
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea5a504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1kjua88b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_dropout: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_hidden_size: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropbout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_hidden_size: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_batches: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_files: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsequence_length: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/1kjua88b\" target=\"_blank\">vivid-sweep-1</a></strong> to <a href=\"https://wandb.ai/freja-thoresen/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files 16\n",
      "validation files 2\n",
      "test files 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17769... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c491c40ba84d423dac772eb0427ca916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_acc</td><td>▁▂▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>training_loss</td><td>█▇▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_acc</td><td>▁▁▅▆▆▇▇▇▇▇▇█▇█▇████▇████▇██▇█▇▇▇█▇▇▇█▇█▇</td></tr><tr><td>validation_loss</td><td>█▅▃▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>training_acc</td><td>0.95447</td></tr><tr><td>training_loss</td><td>17.26991</td></tr><tr><td>validation_acc</td><td>0.94985</td></tr><tr><td>validation_loss</td><td>30.95247</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vivid-sweep-1</strong>: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/1kjua88b\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/runs/1kjua88b</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_184653-1kjua88b/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xf9a2ja4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_dropout: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_hidden_size: 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropbout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_hidden_size: 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_batches: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_files: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsequence_length: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/xf9a2ja4\" target=\"_blank\">avid-sweep-2</a></strong> to <a href=\"https://wandb.ai/freja-thoresen/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files 16\n",
      "validation files 2\n",
      "test files 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36395... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a872221dd0430b89d1e6d27f3a28e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_acc</td><td>▁▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>training_loss</td><td>█▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_acc</td><td>▁▂▄▅▃▄▆▆▆▆▆▆▆▇▇▇▆▇▇█▇████▇▇█▇█▇▇██▇█▇▇▆▇</td></tr><tr><td>validation_loss</td><td>█▅▄▃▅▄▂▃▂▂▃▃▂▂▂▂▂▁▂▁▂▁▁▂▁▂▂▁▂▂▂▂▁▁▂▁▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>training_acc</td><td>0.94691</td></tr><tr><td>training_loss</td><td>17.52424</td></tr><tr><td>validation_acc</td><td>0.94517</td></tr><tr><td>validation_loss</td><td>22.79471</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">avid-sweep-2</strong>: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/xf9a2ja4\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/runs/xf9a2ja4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_193321-xf9a2ja4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8tfu3xdn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_dropout: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_hidden_size: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropbout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_hidden_size: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_batches: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_files: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsequence_length: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/8tfu3xdn\" target=\"_blank\">laced-sweep-3</a></strong> to <a href=\"https://wandb.ai/freja-thoresen/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files 16\n",
      "validation files 2\n",
      "test files 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43751... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b9b22d191946f5a4e28a1c9922f3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_acc</td><td>▁▄▅▅▇▇▇▇█▇▆█▇█▇██▇▇▇▇▇████▇█▇▇██████▇▇██</td></tr><tr><td>training_loss</td><td>█▅▄▄▂▂▂▂▁▂▃▂▂▁▂▁▁▂▁▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_acc</td><td>▁▂▃▄▇▇▇▇▇█████████████▇▇██▇▇██▇█▇▇▇█▇▆▇█</td></tr><tr><td>validation_loss</td><td>█▄▄▃▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁▂▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>training_acc</td><td>0.94284</td></tr><tr><td>training_loss</td><td>19.52981</td></tr><tr><td>validation_acc</td><td>0.96275</td></tr><tr><td>validation_loss</td><td>10.87553</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">laced-sweep-3</strong>: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/8tfu3xdn\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/runs/8tfu3xdn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_200159-8tfu3xdn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nrjfc2rf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_dropout: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_hidden_size: 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropbout: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_hidden_size: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_batches: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_files: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsequence_length: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/nrjfc2rf\" target=\"_blank\">upbeat-sweep-4</a></strong> to <a href=\"https://wandb.ai/freja-thoresen/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files 16\n",
      "validation files 2\n",
      "test files 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 55858... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3533dc19124a0b94956be23b90cdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_acc</td><td>▁▃▅▆▄▆▇▆▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>training_loss</td><td>█▆▅▄▅▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_acc</td><td>▁▇▇████▄█▇██▆█▄▆████▇████▇██▆█▆▃██▄▄████</td></tr><tr><td>validation_loss</td><td>▇▄▃▂▃▃▂█▃▃▂▂▄▂▇▅▂▁▂▁▄▂▁▂▃▃▁▂▅▁▅█▃▂██▂▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>training_acc</td><td>0.95046</td></tr><tr><td>training_loss</td><td>16.94091</td></tr><tr><td>validation_acc</td><td>0.9585</td></tr><tr><td>validation_loss</td><td>8.86648</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">upbeat-sweep-4</strong>: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/nrjfc2rf\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/runs/nrjfc2rf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_202748-nrjfc2rf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z2vkc67m with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_hidden_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropbout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_hidden_size: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_batches: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_files: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsequence_length: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/z2vkc67m\" target=\"_blank\">sweet-sweep-5</a></strong> to <a href=\"https://wandb.ai/freja-thoresen/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files 16\n",
      "validation files 2\n",
      "test files 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 70976... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ffbd6226f745fdbac2f4b14e890e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>training_loss</td><td>█▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_acc</td><td>▁▇██████▇███████████████████████████████</td></tr><tr><td>validation_loss</td><td>█▃▂▂▃▂▂▁▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>training_acc</td><td>0.95486</td></tr><tr><td>training_loss</td><td>19.72941</td></tr><tr><td>validation_acc</td><td>0.95903</td></tr><tr><td>validation_loss</td><td>10.66202</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sweet-sweep-5</strong>: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/z2vkc67m\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/runs/z2vkc67m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_210822-z2vkc67m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u8z9njuq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_dropout: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_hidden_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropbout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_hidden_size: 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_batches: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_files: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsequence_length: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/freja-thoresen/uncategorized/runs/u8z9njuq\" target=\"_blank\">faithful-sweep-6</a></strong> to <a href=\"https://wandb.ai/freja-thoresen/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps\" target=\"_blank\">https://wandb.ai/freja-thoresen/uncategorized/sweeps/lpycscps</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files 16\n",
      "validation files 2\n",
      "test files 2\n"
     ]
    }
   ],
   "source": [
    "def sweep():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        lstm, criterion, optimizer = make_model(config)\n",
    "        train_model(lstm, criterion, optimizer, config)\n",
    "count = 100 # number of runs to execute\n",
    "wandb.agent(sweep_id, function=sweep, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save(\"lstm_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), '../models/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_gen = data_generator(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,5))\n",
    "data, labels = train_gen.__next__()\n",
    "data = data.detach().numpy()[:,0,0]\n",
    "#plt.plot(data[int(len(data)*0.25):int(len(data)*0.75)])\n",
    "plt.plot(data[5000:50000]*1000)\n",
    "\n",
    "if labels.sum() > 0:\n",
    "#plt.plot(labels[int(len(data)*0.25):int(len(data)*0.75)]*1330)\n",
    "    plt.plot(labels[5000:50000]*0.2)\n",
    "#plt.gca().set_ylim([1300,1500])\n",
    "#plt.gca().set_xlim([1610000,len(data)-860000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, len(epoch_validation_loss)-1, len(epoch_validation_loss)), epoch_validation_loss, label='validation loss')\n",
    "plt.plot(np.linspace(0, len(epoch_training_loss)-1, len(epoch_training_loss)), epoch_training_loss, label='training loss')\n",
    "plt.gca().set_ylabel('Loss')\n",
    "plt.gca().set_xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(figure_path + 'validation.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epoch_validation_acc = [acc.cpu() for acc in epoch_validation_acc]\n",
    "epoch_training_acc = [acc.cpu() for acc in epoch_training_acc]\n",
    "test_gen = data_generator(X_test, y_test)\n",
    "\n",
    "plt.plot(np.linspace(0, len(epoch_validation_acc)-1, len(epoch_validation_acc)), epoch_validation_acc, label='validation acc')\n",
    "plt.plot(np.linspace(0, len(epoch_training_acc)-1, len(epoch_training_acc)), epoch_training_acc, label='training acc')\n",
    "plt.gca().set_ylabel('Accuracy')\n",
    "plt.gca().set_xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(figure_path + 'accuracy.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197123a2-9898-448a-9f63-798481c0a298",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09449e88-217c-491c-965d-a785019172e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = test_gen.__next__()\n",
    "#train_predict = lstm(data.cuda())#forward pass\n",
    "train_predict = lstm(data.cuda())#forward pass\n",
    "data_predict = train_predict.data.cpu().numpy() #numpy conversion\n",
    "data_predict[data_predict > 0.5] = 1\n",
    "data_predict[data_predict < 0.5] = 0\n",
    "\n",
    "dataY_plot = labels\n",
    "\n",
    "plt.figure(figsize=(10,6)) #plotting\n",
    "#plt.axvline(x=40000, c='r', linestyle='--') #size of the training set\n",
    "\n",
    "plt.plot(dataY_plot, label='Actual Data') #actual plot\n",
    "plt.plot(data_predict, label='Predicted Data') #predicted plot\n",
    "plt.title('Time-Series Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f4aab342266c55a5f356d03dbdb6a3962f7972b11ff589a1da546ab1d11a116"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fp_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
